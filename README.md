# Data Engineering - Batch Processing using PySpark

![header](https://github.com/acothaha/img/blob/main/batch_processing_pyspark/Batch%20Processing.png?raw=true)

## **Project Details**

The aim of this project is to drill myself in batch processing with PySpark. This is a rather simple yet comprehensive project

that incorporate multiple tools for the purpose of extract, transform and load data then finally visualizing it.


## **Project Datasets**

In this project, I ,for the most part, am handling 3 open datasets that i retrieved from a NYC Taxi & Limousine Commission ([site](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)):

- Yellow Taxi Trip Records:

    This data includes fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts of yellow taxi in NYC

- Green Taxi Trip Records

    This data includes fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts of green taxi in NYC

- Taxi Zone Lookup

    This data includes the information of the name of zones according to their code

## **Tools and Technologies**

- Python (Pandas, GCP SDK, Prefect SDK)
- PySpark
- Docker
- GCP services (BigQuery, Looker)
- Prefect 
- ETL: Extract, Transform, Load Data
- Data Warehouse Concepts
- Data Modeling
- Cloud Computing Concepts
- Bash

## **Project Steps**

### **Step 1: Define scope of the project and necessary resources**

- Identify and search for data needed for the project. Determine the end use cases to prepare the data for (e.g., Analytics table, machine learning, etc.)
- Formulate the data lifecycle.
- Initiate a cloud architecture (GCP, prefect) to be used for the project.
- Commence an environment needed for the project (Docker) (Not yet)




